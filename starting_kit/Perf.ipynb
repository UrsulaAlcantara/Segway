{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dir = './ingestion_program/'  \n",
    "score_dir = './scoring_program/'\n",
    "model_dir = './sample_code_submission/'\n",
    "result_dir = './sample_result_submission/'\n",
    "path.append(model_dir); path.append(problem_dir); path.append(score_dir);\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import read_as_df\n",
    "data_dir = './all_data'\n",
    "data_name = 'xporters'\n",
    "data = read_as_df(data_dir  + '/' + data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import DataManager\n",
    "D = DataManager(data_name, data_dir, replace_missing=True)\n",
    "\n",
    "X_train = D.data['X_train']\n",
    "Y_train = D.data['Y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Imports\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring fonction\n",
    "score_dir = 'scoring_program/'\n",
    "path.append(score_dir)\n",
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model List\n",
    "voter1 = GradientBoostingRegressor()\n",
    "voter2 = RandomForestRegressor()\n",
    "voter3 = DecisionTreeRegressor()\n",
    "\n",
    "model_name = [\n",
    "    'KNeighbors',\n",
    "    'DecisionTree',\n",
    "    'RandomForest',\n",
    "    'GradientBoosting',\n",
    "    'Voting - GB - DT - RF']\n",
    "\n",
    "model_list = [\n",
    "    KNeighborsRegressor(n_neighbors=5),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    VotingRegressor(estimators=[('gb', voter1), ('rf', voter2), ('lr', voter3)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(columns =['perf_tr', 'perf_te'])\n",
    "\n",
    "X_train = D.data['X_train']\n",
    "Y_train = D.data['Y_train']\n",
    "X_entrainement,X_validation,Y_entrainement,Y_validation = train_test_split(X_train,Y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "metric_name, scoring_function = get_metric()\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    M = model_list[i]\n",
    "    M.fit(X_entrainement,Y_entrainement)\n",
    "\n",
    "    print(model_name[i])\n",
    "    scores_train = cross_val_score(M, X_entrainement, Y_entrainement, cv=5, scoring=make_scorer(scoring_function))   \n",
    "    scores_test = cross_val_score(M, X_validation, Y_validation, cv=5, scoring=make_scorer(scoring_function))\n",
    "\n",
    "    data_df.loc[model_name[i]] = [scores_train.mean(), scores_test.mean()]\n",
    "\n",
    "data_df[['perf_tr', 'perf_te']].plot.bar()\n",
    "plt.ylabel(metric_name)\n",
    "plt.title(\"performance des modèles en histogramme\")\n",
    "\n",
    "\n",
    "data_df[['perf_tr', 'perf_te']].plot.line()\n",
    "plt.ylabel(metric_name)\n",
    "plt.title(\"performance des modèles en courbe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
